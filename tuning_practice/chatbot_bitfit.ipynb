{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c153d8",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸€éƒ¨åˆ†BitFitå®æˆ˜\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c8dce15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# åˆ‡æ¢åˆ°å›½å†…é•œåƒç«™ï¼Œé€Ÿåº¦å¿«ä¸”ç¨³å®š\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,DataCollatorForSeq2Seq,TrainingArguments,Trainer\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9adf7a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'input', 'instruction'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk(\"./alpaca_data_zh/\")#è¦å…ˆæŠŠæ•°æ®ds.save_to_diskæ‰èƒ½ç”¨load_from_diskï¼Œç°åœ¨å°±æ˜¯è¿™ä¸ªæƒ…å†µ\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ac2326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomTokenizerFast(name_or_path='Langboat/bloom-1b4-zh', vocab_size=46145, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/bloom-1b4-zh\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34d88e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    Max_LENGTH = 256\n",
    "    input_ids,attention_mask,labels = [],[],[]\n",
    "    instruction = tokenizer(\"\\n\".join(['Human:'+ example[\"instruction\"],example[\"input\"]]).strip() + \"\\n\\nAssistant:\")\n",
    "    response = tokenizer(example[\"output\"]+tokenizer.eos_token)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"])+response[\"input_ids\"]#-100è¡¨ç¤ºä¸ç®¡ï¼Œåªå­¦ç­”æ¡ˆéƒ¨åˆ†\n",
    "    if len(input_ids)>Max_LENGTH:\n",
    "        input_ids = input_ids[:Max_LENGTH]\n",
    "        attention_mask = attention_mask[:Max_LENGTH]\n",
    "        labels = labels[:Max_LENGTH]\n",
    "    return{\n",
    "        \"input_ids\":input_ids,\n",
    "        \"attention_mask\":attention_mask,\n",
    "        \"labels\":labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc7d7c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func,remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43a870ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human:è§£é‡Šä¸ºä»€ä¹ˆä»¥ä¸‹åˆ†æ•°ç­‰åŒäº1/4\\nè¾“å…¥ï¼š4/16\\n\\nAssistant:4/16ç­‰äº1/4æ˜¯å› ä¸ºæˆ‘ä»¬å¯ä»¥çº¦åˆ†åˆ†å­åˆ†æ¯éƒ½é™¤ä»¥ä»–ä»¬çš„æœ€å¤§å…¬çº¦æ•°4ï¼Œå¾—åˆ°ï¼ˆ4Ã·4ï¼‰/ (16Ã·4ï¼‰=1/4ã€‚åˆ†æ•°çš„çº¦åˆ†æ˜¯ç”¨åˆ†å­å’Œåˆ†æ¯é™¤ä»¥ç›¸åŒçš„éé›¶æ•´æ•°ï¼Œæ¥è¡¨ç¤ºåˆ†æ•°çš„ä¸€ä¸ªç›¸åŒçš„å€¼ï¼Œè¿™å› ä¸ºåˆ†æ•°å®é™…ä¸Šè¡¨ç¤ºäº†åˆ†å­é™¤ä»¥åˆ†æ¯ï¼Œæ‰€ä»¥å³ä½¿ä¸¤ä¸ªæ•°åŒæ—¶é™¤ä»¥åŒä¸€ä¸ªéé›¶æ•´æ•°ï¼Œåˆ†æ•°çš„å€¼ä¹Ÿä¸ä¼šæ”¹å˜ã€‚æ‰€ä»¥4/16 å’Œ1/4æ˜¯ä¸¤ç§ä¸åŒçš„ä¹¦å†™å½¢å¼ï¼Œä½†å®ƒä»¬çš„å€¼ç›¸ç­‰ã€‚</s>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_ds[1][\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d081f65",
   "metadata": {},
   "source": [
    "Python 3 ä¸­ï¼Œfilter() è¿”å›çš„æ˜¯ä¸€ä¸ª è¿­ä»£å™¨ï¼ˆIteratorï¼‰ï¼Œè¿­ä»£å™¨éœ€è¦ç±»ä¼¼äºéå†æ“ä½œæ¯”å¦‚listå»æ¿€æ´»æ‰ä¼šä½¿ç”¨ï¼Œè€Œtokenizer.decode(token_ids) å‡½æ•°ï¼Œå®ƒçš„å‚æ•° token_ids æ˜ç¡®è¦æ±‚å¿…é¡»æ˜¯ åºåˆ—ï¼ˆSequenceï¼‰ ç±»å‹ï¼Œæ¯”å¦‚ List[int]ï¼ˆæ•´æ•°åˆ—è¡¨ï¼‰ã€Tensor æˆ–è€… Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "065089f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4/16ç­‰äº1/4æ˜¯å› ä¸ºæˆ‘ä»¬å¯ä»¥çº¦åˆ†åˆ†å­åˆ†æ¯éƒ½é™¤ä»¥ä»–ä»¬çš„æœ€å¤§å…¬çº¦æ•°4ï¼Œå¾—åˆ°ï¼ˆ4Ã·4ï¼‰/ (16Ã·4ï¼‰=1/4ã€‚åˆ†æ•°çš„çº¦åˆ†æ˜¯ç”¨åˆ†å­å’Œåˆ†æ¯é™¤ä»¥ç›¸åŒçš„éé›¶æ•´æ•°ï¼Œæ¥è¡¨ç¤ºåˆ†æ•°çš„ä¸€ä¸ªç›¸åŒçš„å€¼ï¼Œè¿™å› ä¸ºåˆ†æ•°å®é™…ä¸Šè¡¨ç¤ºäº†åˆ†å­é™¤ä»¥åˆ†æ¯ï¼Œæ‰€ä»¥å³ä½¿ä¸¤ä¸ªæ•°åŒæ—¶é™¤ä»¥åŒä¸€ä¸ªéé›¶æ•´æ•°ï¼Œåˆ†æ•°çš„å€¼ä¹Ÿä¸ä¼šæ”¹å˜ã€‚æ‰€ä»¥4/16 å’Œ1/4æ˜¯ä¸¤ç§ä¸åŒçš„ä¹¦å†™å½¢å¼ï¼Œä½†å®ƒä»¬çš„å€¼ç›¸ç­‰ã€‚</s>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x:x!=-100,tokenized_ds[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16bc2f",
   "metadata": {},
   "source": [
    "model.parameters()ï¼šè¿”å›çš„æ˜¯ä¸€ä¸ªä¸ª Tensorï¼ˆå¼ é‡/çŸ©é˜µï¼‰ã€‚\n",
    "\n",
    "æ¯”å¦‚ï¼šä¸€ä¸ªå½¢çŠ¶ä¸º (3, 4) çš„çŸ©é˜µã€‚param.numel()ï¼šæ„æ€æ˜¯ Number of Elementsï¼ˆå…ƒç´ çš„æ•°é‡ï¼‰\n",
    "å¯¹äº (3, 4) çš„çŸ©é˜µï¼Œnumel() è¿”å› 12ã€‚å®ƒæ˜¯åœ¨æ•°â€œç®±å­é‡Œæœ‰å‡ ä¸ªè‹¹æœâ€ã€‚\n",
    "low_cpu_mem_usage=Trueï¼ˆä½ CPU å†…å­˜å ç”¨æ¨¡å¼ï¼‰åŠ è½½æ¨¡å‹èŠ‚çœå†…å­˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "473fcf8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1303111680"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Langboat/bloom-1b4-zh\",low_cpu_mem_usage=True)\n",
    "sum(param.numel() for param in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f33dba61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544768"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_param = 0\n",
    "for name,param in model.named_parameters():\n",
    "    if \"bias\" not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        num_param += param.numel()\n",
    "num_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64ea6305",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./chatbot\",\n",
    "    per_device_eval_batch_size=1,#åœ¨éªŒè¯é˜¶æ®µæ¯å¼ æ˜¾å¡ä¸€æ¬¡è¯»å–å¤šå°‘æ¡æ•°æ®\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,#æ¯å…«æ¬¡æ›´æ–°ä¸€æ¬¡æ¢¯åº¦\n",
    "    logging_steps=10,#æ¯è®­ç»ƒå¤šå°‘æ‰“å°ä¸€æ¬¡æ—¥å¿—ï¼Œä¸»è¦æ˜¯loss\n",
    "    num_train_epochs=1,#æ•´ä¸ªå®Œæ•´è®­ç»ƒé›†å­¦å‡ é\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3043c8ce",
   "metadata": {},
   "source": [
    "Trainerå¹¶æ²¡æœ‰å…‹éš†ä¸€ä¸ªæ–°çš„æ¨¡å‹å»è®­ç»ƒï¼Œè€Œæ˜¯ç›´æ¥ä¿®æ”¹ä½ ä¼ è¿›å»çš„é‚£ä¸ª model å¯¹è±¡\n",
    "æ¯ä¸€æ¬¡ Stepï¼ˆæ¯ä¸€ä¸ª Batchï¼‰ï¼Œmodel é‡Œçš„å‚æ•°ï¼ˆWeightsï¼‰éƒ½åœ¨è¢«æ›´æ–°ã€‚\n",
    "\n",
    "å¦‚æœä½ åœ¨è®­ç»ƒé€”ä¸­ç‚¹äº†â€œåœæ­¢â€æŒ‰é’®ï¼ˆInterrupt Kernelï¼‰ï¼Œå†…å­˜é‡Œçš„ model å°±ä¼šåœç•™åœ¨ä½ ç‚¹å‡»é‚£ä¸€ç¬é—´çš„çŠ¶æ€ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4188af68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vt/2rwrwr1j1hn9724wbbsr7jm80000gn/T/ipykernel_6389/3248672971.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer,padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15567ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼šå¦‚æœæœ‰è‹¹æœåŠ é€Ÿç”¨ mpsï¼Œå¦åˆ™ç”¨ cpu\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "299598f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuzhen/Documents/PEFT Study/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='783' max='3358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 783/3358 1:24:49 < 4:39:38, 0.15 it/s, Epoch 0.23/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.717900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.711400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.675800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.621200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.533700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.589300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.461800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.578100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.590400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.544500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.535600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.618500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.567200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.519400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.513500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.548500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.445100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.475600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.448100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.499100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.485200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.439700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.375100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.395100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.466500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.500900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.577400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.505900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.530300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.395500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.453300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.553700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.583900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.351400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.526800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.528700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.384900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.458700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.574500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.367400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.424200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.398800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.446700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.391200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.410700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.435700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>2.439000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.435900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.341600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.487000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PEFT Study/.venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PEFT Study/.venv/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PEFT Study/.venv/lib/python3.12/site-packages/transformers/trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PEFT Study/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2740\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2738\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PEFT Study/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PEFT Study/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PEFT Study/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbee4ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹ä¿å­˜æˆåŠŸï¼\n",
      "ğŸ’¾ æƒé‡æ–‡ä»¶ä½ç½®: ./chatbot_bitfit_final/bitfit_weights.pt\n",
      "ğŸ“‰ æ–‡ä»¶å¤§å°: 2.12 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# 1. å®šä¹‰ä¿å­˜è·¯å¾„\n",
    "output_dir = \"./chatbot_bitfit_final\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 2. æ ¸å¿ƒé€»è¾‘ï¼šåªæå– requires_grad=True çš„å‚æ•° (ä¹Ÿå°±æ˜¯ Bias)\n",
    "# è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šéå†æ¨¡å‹æ‰€æœ‰å‚æ•°ï¼Œè°åˆšæ‰å‚ä¸äº†è®­ç»ƒï¼Œå°±æŠŠè°æ‹¿å‡ºæ¥\n",
    "trainable_params = {\n",
    "    name: param.cpu() # è®°å¾—æ¬å› CPUï¼Œé˜²æ­¢ä¸‹æ¬¡åŠ è½½æ—¶è®¾å¤‡ä¸å…¼å®¹\n",
    "    for name, param in model.named_parameters() \n",
    "    if param.requires_grad\n",
    "}\n",
    "\n",
    "# 3. ä¿å­˜ä¸º PyTorch æ–‡ä»¶\n",
    "save_path = os.path.join(output_dir, \"bitfit_weights.pt\")\n",
    "torch.save(trainable_params, save_path)\n",
    "\n",
    "# 4. é¡ºä¾¿ä¿å­˜ä¸€ä¸‹ tokenizerï¼Œæ–¹ä¾¿ä»¥åç›´æ¥ç”¨è¿™ä¸ªç›®å½•\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸï¼\")\n",
    "print(f\"ğŸ’¾ æƒé‡æ–‡ä»¶ä½ç½®: {save_path}\")\n",
    "print(f\"ğŸ“‰ æ–‡ä»¶å¤§å°: {os.path.getsize(save_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c188b60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> æ‰‹åŠ¨æ¨ç†:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: è€ƒè¯•æœ‰å“ªäº›æŠ€å·§ï¼Ÿ\n",
      "\n",
      "Assistant: è€ƒè¯•æœ‰è®¸å¤šæŠ€å·§ï¼Œæ¯”å¦‚å¯¹ç­”é¢˜æ—¶é—´çš„å……åˆ†åˆ©ç”¨ï¼šå°½é‡ä¿æŒæ•´å—ç­”é¢˜æ—¶é—´æ¥ç­”é¢˜ï¼›åˆ©ç”¨è‰ç¨¿çº¸è®°å½•ç­”æ¡ˆè¦ç‚¹ç­‰ã€‚æ­¤å¤–ï¼Œè¦åšåˆ°è®¤çœŸé˜…è¯»é¢˜ç›®ä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼Œåœ¨å¬ä¸æ‡‚çš„æƒ…å†µä¸‹ä¸è¦æ€¥äºä¸‹ç¬”ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä¿æŒè‰¯å¥½çŠ¶æ€ï¼Œä¸è¦ç„¦è™‘ï¼ŒåŠªåŠ›è¿æ¥ä¸‹æ¬¡è€ƒè¯•ã€‚\n",
      "\n",
      ">>> Pipelineæ¨ç†:\n",
      "Human: è€ƒè¯•æœ‰å“ªäº›æŠ€å·§ï¼Ÿ\n",
      "\n",
      "Assistant: è€ƒè¯•æŠ€å·§å°±æ˜¯æŒæ¡å­¦ä¹ æ–¹æ³•ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ã€‚å­¦ä¹ æ•ˆç‡é«˜ï¼Œæ‰èƒ½æé«˜å­¦ä¹ æ•ˆæœï¼›åä¹‹ï¼Œå­¦ä¹ æ•ˆæœä¸å¥½ï¼Œå­¦ä¹ æ•ˆç‡è‡ªç„¶ä¹Ÿéš¾ä»¥æé«˜ã€‚è€ƒè¯•æ—¶ï¼Œè€ƒç”Ÿé™¤äº†æŒæ¡è€ƒè¯•æŠ€å·§ï¼Œè¿˜éœ€è¦éµå¾ªå¾ªåºæ¸è¿›çš„å­¦ä¹ åŸåˆ™ã€‚\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# 1. å‡†å¤‡å·¥ä½œï¼šè‡ªåŠ¨å®šè®¾å¤‡ (Macç”¨mps)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "base_path = \"Langboat/bloom-1b4-zh\"\n",
    "save_path = \"./chatbot_bitfit_final\" # ä½ åˆšæ‰ä¿å­˜çš„ç›®å½•\n",
    "\n",
    "# 2. åŠ è½½åº•åº§æ¨¡å‹ + åˆ†è¯å™¨\n",
    "model = AutoModelForCausalLM.from_pretrained(base_path, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "# 3. ã€æ ¸å¿ƒã€‘åŠ è½½ BitFit æƒé‡å¹¶è¦†ç›–\n",
    "# strict=False æ„æ€æ˜¯ï¼šåªæ›´æ–°æ–‡ä»¶é‡Œæœ‰çš„å‚æ•°(Bias)ï¼Œç¼ºçš„å‚æ•°(Weight)å°±ç”¨åº•åº§æ¨¡å‹åŸæ¥çš„ï¼Œä¸æŠ¥é”™ï¼Œmap_locationè¡¨ç¤ºç”¨cpuåŠ è½½æ¨¡å‹\n",
    "model.load_state_dict(torch.load(f\"{save_path}/bitfit_weights.pt\", map_location=\"cpu\"), strict=False)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- æ–¹å¼ä¸€ï¼šæ‰‹åŠ¨æ¨ç† (Manual) ---\n",
    "print(\">>> æ‰‹åŠ¨æ¨ç†:\")\n",
    "ipt = tokenizer(\"Human: è€ƒè¯•æœ‰å“ªäº›æŠ€å·§ï¼Ÿ\\n\\nAssistant: \", return_tensors=\"pt\").to(device)\n",
    "# [0]æ˜¯è§£åŒ…batchï¼Œskip_special_tokensæ˜¯å»æ‰äº†</s>ç­‰ç¬¦å·\n",
    "print(tokenizer.decode(model.generate(**ipt, max_length=512, do_sample=True)[0], skip_special_tokens=True))\n",
    "\n",
    "# --- æ–¹å¼äºŒï¼šPipeline (å°è£…ç‰ˆ) ---\n",
    "print(\"\\n>>> Pipelineæ¨ç†:\")\n",
    "# Macä¸Š device ç›´æ¥ä¼ å­—ç¬¦ä¸² \"mps\" å³å¯\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
    "print(pipe(\"Human: è€ƒè¯•æœ‰å“ªäº›æŠ€å·§ï¼Ÿ\\n\\nAssistant: \", max_new_tokens=256, do_sample=True)[0]['generated_text'])#max_lengthæ˜¯æç¤ºè¯åŠ å›ç­”é•¿åº¦ï¼Œmax_new_tokensæ˜¯å›ç­”é•¿åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7712eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 3\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹æ¨¡å‹è‡ªå¸¦çš„ç”Ÿæˆé…ç½®\n",
    "print(model.generation_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PEFT Study (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
