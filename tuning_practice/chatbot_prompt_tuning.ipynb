{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee46c2e",
   "metadata": {},
   "source": [
    "## 基于prompt——tuning的两种形式，soft和hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ce61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,DataCollatorForSeq2Seq,TrainingArguments,Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f26f358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'input', 'instruction'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk(\"./alpaca_data_zh/\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74e70ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': ['以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。',\n",
       "  '4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。',\n",
       "  '朱利叶斯·凯撒，又称尤利乌斯·恺撒（Julius Caesar）是古罗马的政治家、军事家和作家。他于公元前44年3月15日被刺杀。 \\n\\n根据历史记载，当时罗马元老院里一些参议员联合起来策划了对恺撒的刺杀行动，因为他们担心恺撒的统治将给罗马共和制带来威胁。在公元前44年3月15日（又称“3月的艾达之日”），恺撒去参加元老院会议时，被一群参议员包围并被攻击致死。据记载，他身中23刀，其中一刀最终致命。'],\n",
       " 'input': ['', '输入：4/16', ''],\n",
       " 'instruction': ['保持健康的三个提示。', '解释为什么以下分数等同于1/4', '朱利叶斯·凯撒是如何死亡的？']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5db659e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomTokenizerFast(name_or_path='Langboat/bloom-1b4-zh', vocab_size=46145, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Langboat/bloom-1b4-zh\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a012158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fun(example):\n",
    "    MAX_LENGTH = 256\n",
    "    input_ids,attention_mask,labels = {},{},{}\n",
    "    instruction = \"\\n\".join([\"Human:\" + example[\"instruction\"],example[\"input\"]]).strip() + \"\\n\\nAssistant:\"\n",
    "    instruction = tokenizer(instruction)\n",
    "    response = tokenizer(example[\"output\"] + tokenizer.eos_token)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"]\n",
    "    labels = [-100]*len(instruction[\"input_ids\"]) + response[\"input_ids\"]\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ad6535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 26858\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_fun, remove_columns=ds.column_names)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "944dc854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human:解释为什么以下分数等同于1/4\\n输入：4/16\\n\\nAssistant:4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。</s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_ds[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b0bb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4/16等于1/4是因为我们可以约分分子分母都除以他们的最大公约数4，得到（4÷4）/ (16÷4）=1/4。分数的约分是用分子和分母除以相同的非零整数，来表示分数的一个相同的值，这因为分数实际上表示了分子除以分母，所以即使两个数同时除以同一个非零整数，分数的值也不会改变。所以4/16 和1/4是两种不同的书写形式，但它们的值相等。</s>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x:x!=-100,tokenized_ds[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e11156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Langboat/bloom-1b4-zh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da52a8e",
   "metadata": {},
   "source": [
    "# Prompt tuning\n",
    "## PEFT Step1 配置文件\n",
    "### soft_prompt的配置方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d586810e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTuningConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, peft_version='0.18.0', base_model_name_or_path=None, revision=None, inference_mode=False, num_virtual_tokens=10, token_dim=None, num_transformer_submodules=None, num_attention_heads=None, num_layers=None, modules_to_save=None, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None, tokenizer_kwargs=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PromptTuningConfig,get_peft_model,TaskType,PromptTuningInit,PeftModel\n",
    "config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM,num_virtual_tokens=10)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc87b3",
   "metadata": {},
   "source": [
    "### hard_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca25ee22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, num_virtual_tokens=8, token_dim=None, num_transformer_submodules=None, num_attention_heads=None, num_layers=None, prompt_tuning_init=<PromptTuningInit.TEXT: 'TEXT'>, prompt_tuning_init_text='下面是一段人与机器人的对话。', tokenizer_name_or_path='Langboat/bloom-1b4-zh')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PromptTuningConfig,get_peft_model,TaskType,PromptTuningInit,PeftModel\n",
    "config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM,\n",
    "                            prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "                            prompt_tuning_init_text=\"下面是一段人与机器人的对话。\",\n",
    "                            num_virtual_tokens=len(tokenizer(\"下面是一段人与机器人的对话。\")[\"input_ids\"]),\n",
    "                            tokenizer_name_or_path=\"Langboat/bloom-1b4-zh\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "301d0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "210e304a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): BloomForCausalLM(\n",
       "    (transformer): BloomModel(\n",
       "      (word_embeddings): Embedding(46145, 2048)\n",
       "      (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (h): ModuleList(\n",
       "        (0-23): 24 x BloomBlock(\n",
       "          (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attention): BloomAttention(\n",
       "            (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BloomMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (gelu_impl): BloomGelu()\n",
       "            (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=46145, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEmbedding(\n",
       "      (embedding): Embedding(8, 2048)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(46145, 2048)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a8349d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,384 || all params: 1,303,128,064 || trainable%: 0.0012572824154909767\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce0bce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./chatbot\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=20,\n",
    "    save_total_limit=3  # 只要最新的3个，之前的会被自动删除\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "832faca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vt/2rwrwr1j1hn9724wbbsr7jm80000gn/T/ipykernel_3174/1988846643.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e7f3f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuzhen/Documents/PEFT_Study/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='3358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  82/3358 03:31 < 2:24:22, 0.38 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.670600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.733400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.630500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.525200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.590100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PEFT_Study/.venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PEFT_Study/.venv/lib/python3.12/site-packages/transformers/trainer.py:2679\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2674\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9305a3db",
   "metadata": {},
   "source": [
    "# 加载训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1e47bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Langboat/bloom-1b4-zh\")\n",
    "peft_model = PeftModel.from_pretrained(model=model, model_id=\"./chatbot/checkpoint-80/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64793124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 考试有哪些技巧？\n",
      "\n",
      "Assistant: 考试最重要的就是考前多看书，把历年考试中出现的问题和考点归纳总结，熟悉考试的题型和套路，做到不慌不忙，不怯场，有信心地去闯关。很多同学不知道如何去阅读教材，因为有些学校的教材和市面上的教材不同，没有办法读懂教材，那么建议同学们购买《考前30天预习计划》，里面有详细的解析和讲解。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "peft_model = peft_model.to(device)\n",
    "\n",
    "ipt = tokenizer(\"Human: {}\\n{}\".format(\"考试有哪些技巧？\", \"\").strip() + \"\\n\\nAssistant: \", return_tensors=\"pt\").to(peft_model.device)\n",
    "print(tokenizer.decode(peft_model.generate(**ipt, max_length=128, do_sample=True)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5be20",
   "metadata": {},
   "source": [
    "**下面是peft==0.5.0版本下手动修复得到的，上面是更新到0.18.0情况下可以直接调用的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc95c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始生成...\n",
      "上面提到的几个技巧，考试中还是要用得着。比如说在回答题目的时候，如果说自己不知道怎么回答，就不要着急去回答别人的问题，那样很容易造成时间的分神。等你自己知道怎么回答的时候，就可以直接去找答案，那样就不用去看别人怎么怎么回答了，这样会更好一些。而且还有个问题，可能有人会在最后说一个问题，那个问题我们可能也不一定知道怎么回答。这个时候，我们可以尝试去回忆自己以前回答过的这个问题，看看是否记得，如果是自己想了很久都想不起来的话，那么这个时候，你就需要去找答案，这样的话\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "# 1. 准备输入\n",
    "text = \"Human: {}\\n{}\".format(\"考试有哪些技巧？\", \"\").strip() + \"\\n\\nAssistant: \"\n",
    "ipt = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "input_ids = ipt['input_ids']\n",
    "batch_size = input_ids.shape[0]\n",
    "\n",
    "# 2. 提取 Prompt Embedding (保持不变)\n",
    "#这里存放的就是你训练的那 10 个虚拟 Token 的向量\n",
    "prompt_embeds = peft_model.prompt_encoder.default.embedding.weight\n",
    "prompt_embeds = prompt_embeds.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "#expand(batch_size, -1, -1) 的作用： 把那些维度为大于等于1的地方，“拉伸”到指定大小。\n",
    "\n",
    "# 3. 核心修复：正确获取 Bloom 的 Embedding\n",
    "with peft_model.disable_adapter():\n",
    "    # --- 修改点在这里 ---\n",
    "    # 之前报错是因为写了 peft_model.base_model.model\n",
    "    # 既然报错说 'BloomForCausalLM' 没有 .model，那说明 peft_model.base_model 本身就是我们要找的对象！\n",
    "    base_model = peft_model.base_model \n",
    "    \n",
    "    # 这是一个通用的方法，无论模型内部叫 transformer 还是 model 都能拿到\n",
    "    embedding_layer = base_model.get_input_embeddings()#最初的word_embedding \n",
    "    \n",
    "    # 获取原始文本的 Embedding\n",
    "    raw_embeds = embedding_layer(input_ids)#相当于把input_id放到embedding_layer的forward函数运行得到结果\n",
    "    # --- 修改结束 ---\n",
    "    \n",
    "    # 4. 拼接\n",
    "    final_embeds = torch.cat([prompt_embeds, raw_embeds], dim=1)\n",
    "    \n",
    "    # 5. 构造 Mask\n",
    "    final_mask = torch.cat([\n",
    "        torch.ones((batch_size, prompt_embeds.shape[1])).to(device),\n",
    "        ipt['attention_mask']\n",
    "    ], dim=1)\n",
    "\n",
    "    print(\"开始生成...\")\n",
    "\n",
    "    # 6. 生成\n",
    "    output = peft_model.generate(\n",
    "        inputs_embeds=final_embeds,      \n",
    "        attention_mask=final_mask,       \n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        use_cache=True \n",
    "    )\n",
    "\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PEFT Study (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
