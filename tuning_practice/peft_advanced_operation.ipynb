{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d773113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from peft import LoraConfig,get_peft_model,PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00684dfb",
   "metadata": {},
   "source": [
    "# 1. 自定义模型适配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9640a2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = nn.Sequential(\n",
    "    nn.Linear(10,10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10,2)\n",
    ")\n",
    "net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cbbb1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight\n",
      "0.bias\n",
      "2.weight\n",
      "2.bias\n"
     ]
    }
   ],
   "source": [
    "for name,parm in net1.named_parameters():\n",
    "    print(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8979f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(target_modules=[\"0\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25935133",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = get_peft_model(net1,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e16bd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=10, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc40bc3e",
   "metadata": {},
   "source": [
    "# 2. 多适配器加载与切换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1177a9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = nn.Sequential(\n",
    "    nn.Linear(10,10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10,2)\n",
    ")\n",
    "net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "725c39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = LoraConfig(target_modules=[\"0\"])\n",
    "model2 = get_peft_model(net2,config1)\n",
    "model2.save_pretrained(\"./loraA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5babeb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuzhen/Documents/PEFT_Study/.venv/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/Users/liuzhen/Documents/PEFT_Study/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config2 = LoraConfig(target_modules=[\"2\"])\n",
    "model2 = get_peft_model(net2, config2)\n",
    "model2.save_pretrained(\"./loraB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9923aaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "          (loraA): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "          (loraA): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "          (loraA): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=2, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=2, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = PeftModel.from_pretrained(net2, model_id=\"./loraA/\", adapter_name=\"loraA\")\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "131345b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "          (loraA): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "          (loraA): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "          (loraA): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=2, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "          (loraB): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "          (loraB): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=2, bias=False)\n",
       "          (loraB): Linear(in_features=8, out_features=2, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_adapter(\"./loraB/\", adapter_name=\"loraB\")\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50505ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loraA'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c26a652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8964,  2.6760]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(torch.arange(0, 10).view(1, 10).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f77e7465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.0.base_layer.weight Parameter containing:\n",
      "tensor([[-0.1441,  0.2309,  0.2151, -0.3134, -0.0033,  0.2736, -0.1951,  0.1742,\n",
      "          0.1468,  0.1074],\n",
      "        [-0.2579, -0.0858,  0.1584, -0.0578, -0.2392, -0.0630,  0.2318, -0.2393,\n",
      "         -0.0062,  0.0786],\n",
      "        [ 0.1186, -0.2464, -0.2950, -0.0779, -0.0255, -0.1746,  0.2648, -0.1534,\n",
      "          0.1297,  0.1081],\n",
      "        [ 0.1264,  0.2960, -0.1411,  0.2337,  0.3121,  0.2827, -0.0189,  0.0353,\n",
      "          0.0972, -0.0306],\n",
      "        [ 0.0834,  0.1453,  0.0923,  0.0661,  0.1951,  0.1751, -0.2713, -0.2658,\n",
      "          0.2839,  0.1610],\n",
      "        [ 0.1076, -0.0019,  0.2880, -0.0242,  0.1195, -0.1445,  0.1648,  0.2779,\n",
      "          0.1720,  0.2818],\n",
      "        [ 0.1466,  0.1509,  0.0417,  0.1879, -0.2056,  0.2546,  0.0467,  0.0109,\n",
      "          0.0031,  0.2971],\n",
      "        [-0.1008, -0.0935, -0.0917, -0.2641,  0.2559,  0.1507,  0.0146,  0.2908,\n",
      "          0.2180,  0.2246],\n",
      "        [ 0.0124, -0.2925, -0.1521,  0.1501,  0.2793,  0.0559,  0.1669, -0.0209,\n",
      "         -0.2288, -0.1736],\n",
      "        [ 0.1368, -0.0184,  0.1347, -0.0802,  0.0810, -0.2498,  0.2031, -0.0067,\n",
      "         -0.1409,  0.0590]])\n",
      "base_model.model.0.base_layer.bias Parameter containing:\n",
      "tensor([ 0.0038,  0.1332, -0.2285,  0.1663, -0.0078, -0.0013,  0.0308, -0.2820,\n",
      "        -0.2398, -0.1104])\n",
      "base_model.model.0.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.2178,  0.2306,  0.0794, -0.1270,  0.1480,  0.2074, -0.2412,  0.2980,\n",
      "         -0.1777, -0.2631],\n",
      "        [-0.0438, -0.1034, -0.1278,  0.1640, -0.0917,  0.0510, -0.0373,  0.1681,\n",
      "         -0.0907, -0.1401],\n",
      "        [ 0.1648, -0.0712,  0.1840,  0.1282,  0.1466, -0.1059,  0.2388,  0.2335,\n",
      "         -0.1547, -0.1923],\n",
      "        [ 0.2170, -0.1823, -0.0702,  0.2079,  0.2208, -0.2819, -0.2387,  0.2242,\n",
      "          0.0031, -0.0693],\n",
      "        [-0.2563, -0.0610, -0.2535, -0.1673,  0.2976, -0.0215,  0.2833,  0.0851,\n",
      "         -0.1287,  0.1863],\n",
      "        [ 0.1411,  0.2172, -0.3011, -0.0116, -0.1277,  0.2693, -0.0996,  0.1221,\n",
      "          0.2727,  0.1944],\n",
      "        [-0.2993, -0.0118,  0.1123, -0.1639,  0.2146, -0.2394,  0.0210,  0.0242,\n",
      "         -0.1681, -0.1637],\n",
      "        [-0.1616,  0.0259,  0.1986, -0.2763,  0.0847,  0.0945,  0.2363, -0.0976,\n",
      "          0.2443, -0.1287]])\n",
      "base_model.model.0.lora_A.loraA.weight Parameter containing:\n",
      "tensor([[ 0.2178,  0.2306,  0.0794, -0.1270,  0.1480,  0.2074, -0.2412,  0.2980,\n",
      "         -0.1777, -0.2631],\n",
      "        [-0.0438, -0.1034, -0.1278,  0.1640, -0.0917,  0.0510, -0.0373,  0.1681,\n",
      "         -0.0907, -0.1401],\n",
      "        [ 0.1648, -0.0712,  0.1840,  0.1282,  0.1466, -0.1059,  0.2388,  0.2335,\n",
      "         -0.1547, -0.1923],\n",
      "        [ 0.2170, -0.1823, -0.0702,  0.2079,  0.2208, -0.2819, -0.2387,  0.2242,\n",
      "          0.0031, -0.0693],\n",
      "        [-0.2563, -0.0610, -0.2535, -0.1673,  0.2976, -0.0215,  0.2833,  0.0851,\n",
      "         -0.1287,  0.1863],\n",
      "        [ 0.1411,  0.2172, -0.3011, -0.0116, -0.1277,  0.2693, -0.0996,  0.1221,\n",
      "          0.2727,  0.1944],\n",
      "        [-0.2993, -0.0118,  0.1123, -0.1639,  0.2146, -0.2394,  0.0210,  0.0242,\n",
      "         -0.1681, -0.1637],\n",
      "        [-0.1616,  0.0259,  0.1986, -0.2763,  0.0847,  0.0945,  0.2363, -0.0976,\n",
      "          0.2443, -0.1287]])\n",
      "base_model.model.0.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "base_model.model.0.lora_B.loraA.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "base_model.model.2.base_layer.weight Parameter containing:\n",
      "tensor([[ 0.0066, -0.0178, -0.1820,  0.1391,  0.0800,  0.0195, -0.1274, -0.2103,\n",
      "          0.2815,  0.2301],\n",
      "        [ 0.1182,  0.1358, -0.0852,  0.0596,  0.0719,  0.1938,  0.3047, -0.1186,\n",
      "         -0.3048,  0.0325]])\n",
      "base_model.model.2.base_layer.bias Parameter containing:\n",
      "tensor([ 0.0959, -0.0597])\n",
      "base_model.model.2.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.1842,  0.1213,  0.1324, -0.0506, -0.2246, -0.1917, -0.1656,  0.0725,\n",
      "          0.2134,  0.0152],\n",
      "        [-0.0037, -0.0737, -0.0253, -0.2338, -0.2508,  0.2290,  0.0704,  0.0321,\n",
      "         -0.1433,  0.0350],\n",
      "        [-0.3138,  0.0555,  0.2705,  0.0652,  0.0637,  0.2717, -0.2079,  0.2260,\n",
      "         -0.2609, -0.2918],\n",
      "        [ 0.0789,  0.2785,  0.1238,  0.1064,  0.1079,  0.1950,  0.2983,  0.0288,\n",
      "          0.2837,  0.2909],\n",
      "        [-0.1271,  0.2395,  0.1488,  0.1903, -0.1751, -0.2169, -0.1503, -0.0356,\n",
      "         -0.1176,  0.2601],\n",
      "        [-0.2251,  0.1870, -0.1497,  0.0616,  0.2074,  0.2213, -0.0855,  0.1341,\n",
      "          0.0869,  0.1186],\n",
      "        [-0.1704,  0.1232,  0.2961,  0.2688, -0.1614, -0.0950,  0.2707,  0.1393,\n",
      "         -0.0276, -0.2408],\n",
      "        [-0.2021,  0.2681,  0.3143,  0.1942,  0.3060, -0.1597,  0.0493,  0.1830,\n",
      "         -0.2947, -0.1993]])\n",
      "base_model.model.2.lora_A.loraB.weight Parameter containing:\n",
      "tensor([[ 0.1842,  0.1213,  0.1324, -0.0506, -0.2246, -0.1917, -0.1656,  0.0725,\n",
      "          0.2134,  0.0152],\n",
      "        [-0.0037, -0.0737, -0.0253, -0.2338, -0.2508,  0.2290,  0.0704,  0.0321,\n",
      "         -0.1433,  0.0350],\n",
      "        [-0.3138,  0.0555,  0.2705,  0.0652,  0.0637,  0.2717, -0.2079,  0.2260,\n",
      "         -0.2609, -0.2918],\n",
      "        [ 0.0789,  0.2785,  0.1238,  0.1064,  0.1079,  0.1950,  0.2983,  0.0288,\n",
      "          0.2837,  0.2909],\n",
      "        [-0.1271,  0.2395,  0.1488,  0.1903, -0.1751, -0.2169, -0.1503, -0.0356,\n",
      "         -0.1176,  0.2601],\n",
      "        [-0.2251,  0.1870, -0.1497,  0.0616,  0.2074,  0.2213, -0.0855,  0.1341,\n",
      "          0.0869,  0.1186],\n",
      "        [-0.1704,  0.1232,  0.2961,  0.2688, -0.1614, -0.0950,  0.2707,  0.1393,\n",
      "         -0.0276, -0.2408],\n",
      "        [-0.2021,  0.2681,  0.3143,  0.1942,  0.3060, -0.1597,  0.0493,  0.1830,\n",
      "         -0.2947, -0.1993]])\n",
      "base_model.model.2.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "base_model.model.2.lora_B.loraB.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model2.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ed4d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model2.named_parameters():\n",
    "    if name in [\"base_model.model.0.lora_A.loraA.weight\", \"base_model.model.0.lora_B.loraA.weight\"]:\n",
    "        param.data = torch.ones_like(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64644c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 77.5160, 149.8932]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(torch.arange(0, 10).view(1, 10).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1614c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.set_adapter(\"loraB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5785a6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loraB'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "917a5dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8964,  2.6760]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(torch.arange(0,10).view(1,10).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351cce90",
   "metadata": {},
   "source": [
    "# 禁用适配器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b55a64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.set_adapter(\"loraA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b8ac38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 77.5160, 149.8932]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(torch.arange(0,10).view(1,10).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afd79e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8964,  2.6760]])\n"
     ]
    }
   ],
   "source": [
    "with model2.disable_adapter():\n",
    "    print(model2(torch.arange(0,10).view(1,10).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a7922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PEFT Study (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
