import torch
from transformers import AutoModelForSeq2SeqLM
from peft import LoraConfig, get_peft_model, TaskType

# --- 1. éªŒè¯ç¯å¢ƒæ ¸å¿ƒ ---
print(f"python version: {torch.__version__}")

# å…³é”®æ£€æŸ¥ï¼šMac çš„ GPU åŠ é€Ÿæ˜¯å¦å¯ç”¨
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("âœ… æˆåŠŸï¼æ£€æµ‹åˆ° MPS (Metal) åŠ é€Ÿï¼Œå°†ä½¿ç”¨ Mac GPU è¿›è¡Œè®­ç»ƒã€‚")
else:
    device = torch.device("cpu")
    print("âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° MPSï¼Œå°†ä½¿ç”¨ CPU (é€Ÿåº¦ä¼šå¾ˆæ…¢)ã€‚")

print("-" * 30)

# --- 2. å¤ç°ä¹‹å‰çš„ä»£ç  ---
print("æ­£åœ¨å°è¯•åŠ è½½æ¨¡å‹ (é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…)...")

model_name_or_path = "bigscience/mt0-large"

# é…ç½® LoRA
peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM, 
    inference_mode=False, 
    r=8, 
    lora_alpha=32, 
    lora_dropout=0.1
)

try:
    # åŠ è½½æ¨¡å‹
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
    
    # æŠŠæ¨¡å‹ç§»åŠ¨åˆ° Mac çš„ GPU ä¸Š (è¿™æ­¥å¾ˆé‡è¦ï¼)
    model.to(device)
    
    # åŠ è½½ LoRA
    model = get_peft_model(model, peft_config)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°é‡ï¼Œçœ‹çœ‹çœäº†å¤šå°‘å†…å­˜
    model.print_trainable_parameters()
    
    print("\nğŸ‰ æ­å–œï¼ç¯å¢ƒé…ç½®å®Œç¾ï¼Œä»£ç å¤ç°æˆåŠŸï¼")
    
except Exception as e:
    print(f"\nâŒ å‡ºé”™äº†: {e}")